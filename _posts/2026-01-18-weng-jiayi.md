---
layout: post
title: weng jiayi
date: 2026-01-18 16:29 +0800
description:
categories: []
tags: []
pin: false
---

### Reward hacking 
在强化学习中，模型学会最大化奖励函数，却以违背设计初衷或现实目标的方式“取巧”。
这种现象通常源于奖励设计不完备,模型发现了奖励函数中的漏洞或代理目标。在大语言模型和RLHF中, reward hacking 可能表现为表面上符合偏好评分、但实际内容低质或不真实。因此,合理的奖励建模、约束机制 和人工监督对避免 reward hacking 至关重要。
### overreact

trade-off  取舍

通用agi 堆RL或者说堆Pre-train 堆算力然后scale up规模化

infra cycle time 基础设施迭代时间 

### LMsysLMsys
是一个由学术界主导的开源组织，致力于构建和评测大语言模型的系统与基础设施。它最知名的项目包括ChatbotArena，通过真实用户对战式投票来评估不同大模型的综合能力。LMsys强调真实交互、可复现评测和系统层面的可扩展性，对大模型对齐与评测产生了重要影响。


迭代速度和成功率

## 马尔可夫过程
马尔可夫过程是一类随机过程，其核心假设是未来状态只依赖于当前状态，而与更早的历史无关，这被称为马尔可
夫性。它通常用状态集合和状态转移概率来描述系统的演化过程。确定性的马尔可夫过程是一种特殊情况：在给定
当前状态（和动作）后，下一状态是唯一确定的，而不是由概率分布采样得到。